Directory structure:
└── gaze and phone detection/
    ├── AffineTransformer.py
    ├── EyeballDetector.py
    ├── GazeProcessor.py
    ├── VisualizationOptions.py
    ├── face_landmarker.task
    ├── face_model.py
    ├── helper.py
    ├── landmarks.py
    ├── main.py
    ├── requirements.txt
    ├── resnet.py
    ├── resnet34.pt
    ├── yolo11l.pt
    └── cheating_logs/
        └── log.txt

================================================
File: AffineTransformer.py
================================================
# -----------------------------------------------------------------------------------
# Company: TensorSense
# Project: LaserGaze
# File: AffineTransformer.py
# Description: This class is designed to calculate and apply affine transformations
#              between two sets of 3D points, typically used for mapping points from
#              one coordinate system to another. It supports scaling based on defined
#              horizontal and vertical points, estimating an affine transformation matrix,
#              and converting points between the two coordinate systems using the matrix.
#              It is especially useful in projects involving facial recognition,
#              augmented reality, or any application where precise spatial transformations
#              are needed between different 3D models.
# Author: Sergey Kuldin
# -----------------------------------------------------------------------------------

import numpy as np
import cv2

class AffineTransformer:
    """
    A class to calculate and manage affine transformations between two 3D point sets.
    This class allows for precise spatial alignment of 3D models based on calculated
    scale factors and transformation matrices, facilitating tasks such as facial
    landmark transformations or other applications requiring model alignment.
    """

    def __init__(self, m1_points, m2_points, m1_hor_points, m1_ver_points, m2_hor_points, m2_ver_points):
        """
        Initializes the transformer by calculating the scale factor and the affine transformation matrix.

        Args:
        - m1_points (np.array): Numpy array of the first set of 3D points.
        - m2_points (np.array): Numpy array of the second set of 3D points to which the first set is aligned.
        - m1_hor_points (np.array): Horizontal reference points from the first model used to calculate scaling.
        - m1_ver_points (np.array): Vertical reference points from the first model used to calculate scaling.
        - m2_hor_points (np.array): Horizontal reference points from the second model used to calculate scaling.
        - m2_ver_points (np.array): Vertical reference points from the second model used to calculate scaling.
        """
        self.scale_factor = self._get_scale_factor(
            np.array(m1_hor_points),
            np.array(m1_ver_points),
            np.array(m2_hor_points),
            np.array(m2_ver_points)
        )

        scaled_m2_points = m2_points * self.scale_factor

        retval, M, inliers = cv2.estimateAffine3D(m1_points, scaled_m2_points)
        if retval:
            self.success = True
            self.transform_matrix = M
        else:
            self.success = False
            self.transform_matrix = None

    def _get_scale_factor(self, m1_hor_points, m1_ver_points, m2_hor_points, m2_ver_points):
        """
        Calculates the scale factor between two sets of reference points (horizontal and vertical).

        Args:
        - m1_hor_points (np.array): Horizontal reference points from the first model.
        - m1_ver_points (np.array): Vertical reference points from the first model.
        - m2_hor_points (np.array): Horizontal reference points from the second model.
        - m2_ver_points (np.array): Vertical reference points from the second model.

        Returns:
        - float: The calculated uniform scale factor to apply.
        """
        m1_width = np.linalg.norm(m1_hor_points[0] - m1_hor_points[1])
        m1_height = np.linalg.norm(m1_ver_points[0] - m1_ver_points[1])
        m2_width = np.linalg.norm(m2_hor_points[0] - m2_hor_points[1])
        m2_height = np.linalg.norm(m2_ver_points[0] - m2_ver_points[1])
        scale_width = m1_width / m2_width
        scale_height = m1_height / m2_height
        return (scale_width + scale_height) / 2

    def to_m2(self, m1_point):
        """
        Transforms a point from the first model space to the second model space using the affine transformation matrix.

        Args:
        - m1_point (np.array): The point in the first model's coordinate space.

        Returns:
        - np.array or None: Transformed point in the second model's space if the transformation was successful; otherwise None.
        """
        if self.success:
            m1_point_homogeneous = np.append(m1_point, 1)  # Convert to homogeneous coordinates
            return np.dot(self.transform_matrix, m1_point_homogeneous) / self.scale_factor
        else:
            return None

    def to_m1(self, m2_point):
        """
        Transforms a point from the second model space back to the first model space using the inverse of the affine transformation matrix.

        Args:
        - m2_point (np.array): The point in the second model's coordinate space.

        Returns:
        - np.array or None: Transformed point back in the first model's space if the transformation was successful; otherwise None.
        """
        if self.success:
            affine_transform_4x4 = np.vstack([self.transform_matrix, [0, 0, 0, 1]])
            inverse_affine_transform = np.linalg.inv(affine_transform_4x4)
            m2_point_homogeneous = np.append(m2_point * self.scale_factor, 1)  # Convert to homogeneous coordinates
            m1_point_homogeneous = np.dot(inverse_affine_transform, m2_point_homogeneous)

            # Convert back to non-homogeneous coordinates
            return (m1_point_homogeneous[:3] / m1_point_homogeneous[3])
        else:
            return None



================================================
File: EyeballDetector.py
================================================
# -----------------------------------------------------------------------------------
# Company: TensorSense
# Project: LaserGaze
# File: EyeballDetector.py
# Description: The EyeballDetector class is designed to accurately detect and estimate
#              the center and radius of an eyeball's sphere using a set of three-dimensional
#              points. It utilizes a confidence-based approach to continually refine these
#              estimations. The detector initializes with default parameters, including
#              assumptions about the eye's initial center and radius, and adjusts these
#              based on incoming data points. It incorporates a dynamic update mechanism
#              that relies on a minimum confidence threshold to initially detect the center
#              and continues to refine its estimations until a higher, reasonable confidence
#              level is achieved or the update period exceeds a specified threshold. This
#              class is particularly useful in applications requiring precise eye tracking
#              and analysis. The implementation is designed to be flexible, allowing
#              customization of key parameters to suit different accuracy and performance needs.
# Author: Sergey Kuldin
# -----------------------------------------------------------------------------------


import numpy as np
from scipy.optimize import minimize
import time

class EyeballDetector:
    def __init__(self, initial_eye_center,
                 initial_eye_radius=0.02,
                 min_confidence=0.995,
                 reasonable_confidence=0.997,
                 points_threshold=300,
                 points_history_size=400,
                 refresh_time_threshold=10000):
        """
        Initializes the eyeball detector with customizable parameters for detecting the eye's sphere.

        Args:
        - initial_eye_center (np.array): Initial assumption about the eye's center.
        - initial_eye_radius (float): Initial assumption about the eye's radius.
        - min_confidence (float): Minimum confidence to consider the center detected.
        - reasonable_confidence (float): Confidence threshold to stop updating the center and radius.
        - points_threshold (int): Number of points required to start estimation.
        - points_history_size (int): Maximum size of the queue of collected points for calculating.
        - refresh_time_threshold (int): Time in milliseconds to refresh the detection state.
        """
        self.eye_center = np.array(initial_eye_center)
        self.eye_radius = initial_eye_radius
        self.min_confidence = min_confidence
        self.reasonable_confidence = reasonable_confidence
        self.points_threshold = points_threshold
        self.points_history_size = points_history_size
        self.refresh_time_threshold = refresh_time_threshold
        self.points_for_eye_center = None
        self.current_confidence = 0.0
        self.center_detected = False
        self.search_completed = False
        self.last_update_time = int(time.time() * 1000)

    def update(self, new_points, timestamp_ms):
        """
        Updates the detection of the eye's sphere center and radius based on current points and confidence.

        Args:
        - new_points (np.array): New points to add to detection set.
        - timestamp_ms (int): The current frame's timestamp in milliseconds.
        """

        if self.points_for_eye_center is not None:
            self.points_for_eye_center = np.concatenate((self.points_for_eye_center, new_points), axis=0)[-self.points_history_size:]
        else:
            self.points_for_eye_center = new_points

        if len(self.points_for_eye_center) >= self.points_threshold and not self.search_completed:
            center, radius, confidence = self._solve_for_sphere(self.points_for_eye_center)

            if confidence and confidence > self.current_confidence:
                self.eye_center = center
                self.eye_radius = radius
                self.current_confidence = confidence
                self.last_update_time = timestamp_ms

                if confidence >= self.min_confidence:
                    self.center_detected = True
                if confidence >= self.reasonable_confidence:
                    self.search_completed = True  # Indicate that the search can be concluded

            # Reset detection if too much time has passed without an update
            if (timestamp_ms - self.last_update_time) > self.refresh_time_threshold:
                self.search_completed = False

    def _solve_for_sphere(self, points, radius_bounds=(0.015, 0.025)):
        """
        Solves for the sphere's center and radius given a set of points.

        Args:
        - points (np.array): Array of points.
        - radius_bounds (tuple): Bounds for the sphere's radius (min_radius, max_radius).

        Returns:
        - tuple: The center (x, y, z), radius of the sphere, and the confidence of the solution.
        """
        def objective(params, points):
            center, R = params[:3], params[3]
            residuals = np.linalg.norm(points - center, axis=1) - R
            return np.sum(residuals**2)

        bounds = [(None, None), (None, None), (None, None), radius_bounds]
        result = minimize(objective, np.append(self.eye_center, self.eye_radius), args=(points,), bounds=bounds)

        if radius_bounds[0] <= result.x[3] <= radius_bounds[1]:
            center = result.x[:3]
            radius = result.x[3]
            confidence = 1 / (1 + result.fun)  # Inverse of loss
            return center, radius, confidence
        else:
            return None, None, None

    def reset(self):
        """
        Resets the detector to initial values and states.
        """
        self.points_for_eye_center = None
        self.current_confidence = 0.0
        self.center_detected = False
        self.search_completed = False
        self.last_update_time = int(time.time() * 1000)



================================================
File: GazeProcessor.py
================================================
import mediapipe as mp
import cv2
import time
import datetime
import os
import numpy as np
import math
from landmarks import *
from face_model import *
from AffineTransformer import AffineTransformer
from EyeballDetector import EyeballDetector
from ultralytics import YOLO

model_path = "./face_landmarker.task"
BaseOptions = mp.tasks.BaseOptions
FaceLandmarker = mp.tasks.vision.FaceLandmarker
FaceLandmarkerOptions = mp.tasks.vision.FaceLandmarkerOptions
VisionRunningMode = mp.tasks.vision.RunningMode

class GazeRegion:
    def __init__(self):
        # Define angles in degrees
        self.center_horizontal = 30  # Acceptable horizontal angle for center viewing
        self.center_vertical = 20    # Acceptable vertical angle for center viewing
        self.peripheral_horizontal = 45  # Max acceptable horizontal angle
        self.peripheral_vertical = 30    # Max acceptable vertical angle
        
        # Define duration thresholds (in frames)
        self.peripheral_duration_threshold = 20  # How long gaze can stay in peripheral vision
        self.outside_duration_threshold = 10      # How long before triggering warning for outside viewing

class GazeProcessor:
    def __init__(self, camera_idx=0, callback=None, visualization_options=None):
        self.camera_idx = camera_idx
        self.callback = callback
        self.vis_options = visualization_options
        self.left_detector = EyeballDetector(DEFAULT_LEFT_EYE_CENTER_MODEL)
        self.right_detector = EyeballDetector(DEFAULT_RIGHT_EYE_CENTER_MODEL)
        self.options = FaceLandmarkerOptions(
            base_options=BaseOptions(model_asset_path=model_path),
            running_mode=VisionRunningMode.VIDEO
        )
        self.smartphone_detector = YOLO('yolo11l.pt')
        self.phone_detection_frames = 0
        self.phone_detection_threshold = 3
        self.gaze_history_left = []
        self.gaze_history_right = []
        self.history_length = 30
        self.gaze_region = GazeRegion()

    def calculate_gaze_angle(self, vector):
        norm = np.linalg.norm(vector)
        if norm < 1e-6:
            return 0, 0
        normalized = vector / norm
        horizontal_angle = np.degrees(np.arcsin(normalized[0]))
        vertical_angle = np.degrees(np.arcsin(normalized[1]))
        return horizontal_angle, vertical_angle


    def check_gaze_zone(self, h_angle, v_angle):
        if (abs(h_angle) <= self.gaze_region.center_horizontal and 
            abs(v_angle) <= self.gaze_region.center_vertical):
            return "CENTER"
        elif (abs(h_angle) <= self.gaze_region.peripheral_horizontal and 
              abs(v_angle) <= self.gaze_region.peripheral_vertical):
            return "PERIPHERAL"
        else:
            return "OUTSIDE"

    def analyze_gaze_pattern(self, history):
        if not history:
            return False
        
        consecutive_peripheral = 0
        consecutive_outside = 0
        
        for zone in reversed(history):
            if zone == "PERIPHERAL":
                consecutive_peripheral += 1
            elif zone == "OUTSIDE":
                consecutive_outside += 1
            else:
                consecutive_peripheral = 0
                consecutive_outside = 0
                
        return (consecutive_peripheral >= self.gaze_region.peripheral_duration_threshold or 
                consecutive_outside >= self.gaze_region.outside_duration_threshold)

    def is_gaze_out_of_bounds(self, vector, is_left_eye=True):
        horizontal_angle, vertical_angle = self.calculate_gaze_angle(vector)
        gaze_zone = self.check_gaze_zone(horizontal_angle, vertical_angle)
        
        history = self.gaze_history_left if is_left_eye else self.gaze_history_right
        history.append(gaze_zone)
        
        if len(history) > self.history_length:
            history = history[-self.history_length:]
        
        if is_left_eye:
            self.gaze_history_left = history
        else:
            self.gaze_history_right = history
            
        return self.analyze_gaze_pattern(history)

    def draw_gaze_regions(self, frame, head_center):
        h, w = frame.shape[:2]
        center_x, center_y = head_center

        # Draw center zone
        center_w = int(w * math.tan(math.radians(self.gaze_region.center_horizontal)))
        center_h = int(h * math.tan(math.radians(self.gaze_region.center_vertical)))
        cv2.rectangle(frame,
                     (center_x - center_w, center_y - center_h),
                     (center_x + center_w, center_y + center_h),
                     self.vis_options.color, self.vis_options.box_thickness)
        
        # Draw peripheral zone
        periph_w = int(w * math.tan(math.radians(self.gaze_region.peripheral_horizontal)))
        periph_h = int(h * math.tan(math.radians(self.gaze_region.peripheral_vertical)))
        cv2.rectangle(frame,
                     (center_x - periph_w, center_y - periph_h),
                     (center_x + periph_w, center_y + periph_h),
                     self.vis_options.peripheral_color, self.vis_options.box_thickness)

    def draw_gaze_vector(self, frame, start_point, gaze_vector, is_out_of_bounds):
        scale = 100
        end_point = (
            int(start_point[0] + gaze_vector[0] * scale),
            int(start_point[1] + gaze_vector[1] * scale)
        )
        color = self.vis_options.warning_color if is_out_of_bounds else self.vis_options.color
        cv2.arrowedLine(frame, start_point, end_point, color, self.vis_options.line_thickness)

    def draw_gaze_status(self, frame, left_status, right_status):
        h = frame.shape[0]
        cv2.putText(frame, f"Left Eye: {left_status}", 
                    (10, h - 60), cv2.FONT_HERSHEY_SIMPLEX, 
                    self.vis_options.text_scale, self.vis_options.color, 2)
        cv2.putText(frame, f"Right Eye: {right_status}", 
                    (10, h - 30), cv2.FONT_HERSHEY_SIMPLEX, 
                    self.vis_options.text_scale, self.vis_options.color, 2)

    def log_cheating_event(self, reason, frame):
        timestamp = datetime.datetime.now().strftime("%Y-%m-%d_%H-%M-%S")
        folder = "cheating_logs"
        os.makedirs(folder, exist_ok=True)
        filename = os.path.join(folder, f"{timestamp}_{reason.replace(' ', '_')}.jpg")
        cv2.imwrite(filename, frame)
        with open(os.path.join(folder, "log.txt"), "a") as f:
            f.write(f"{timestamp}: {reason}\n")

    async def start(self):
        with FaceLandmarker.create_from_options(self.options) as landmarker:
            cap = cv2.VideoCapture(self.camera_idx)
            while cap.isOpened():
                success, frame = cap.read()
                if not success:
                    continue

                # Phone detection
                detections = self.smartphone_detector.predict(frame, classes=[67], stream=True, verbose=False)
                phone_detected = False
                for detection in detections:
                    if detection.boxes:
                        phone_detected = True
                        frame = detection.plot()
                if phone_detected:
                    self.phone_detection_frames += 1
                else:
                    self.phone_detection_frames = 0
                if self.phone_detection_frames >= self.phone_detection_threshold:
                    self.log_cheating_event("Phone detected", frame)
                    self.phone_detection_frames = 0

                # Face landmark detection
                timestamp_ms = int(time.time() * 1000)
                mp_image = mp.Image(image_format=mp.ImageFormat.SRGB, data=frame)
                face_landmarker_result = landmarker.detect_for_video(mp_image, timestamp_ms)

                if face_landmarker_result.face_landmarks:
                    lms_s = np.array([[lm.x, lm.y, lm.z] for lm in face_landmarker_result.face_landmarks[0]])
                    lms_2 = (lms_s[:, :2] * [frame.shape[1], frame.shape[0]]).round().astype(int)
                    
                    # Calculate head center for visualization
                    nose_tip = lms_2[NOSE_TIP]
                    head_center = (nose_tip[0], nose_tip[1])

                    if self.vis_options:
                        self.draw_gaze_regions(frame, head_center)

                    # Setup affine transformer
                    mp_hor_pts = [lms_s[i] for i in OUTER_HEAD_POINTS]
                    mp_ver_pts = [lms_s[i] for i in [NOSE_BRIDGE, NOSE_TIP]]
                    model_hor_pts = OUTER_HEAD_POINTS_MODEL
                    model_ver_pts = [NOSE_BRIDGE_MODEL, NOSE_TIP_MODEL]
                    at = AffineTransformer(lms_s[BASE_LANDMARKS, :], BASE_FACE_MODEL,
                                         mp_hor_pts, mp_ver_pts, model_hor_pts, model_ver_pts)

                    # Process eyes
                    indices_left = LEFT_IRIS + ADJACENT_LEFT_EYELID_PART
                    left_eye_points = lms_s[indices_left, :]
                    left_eye_points_model = [at.to_m2(p) for p in left_eye_points]
                    self.left_detector.update(left_eye_points_model, timestamp_ms)

                    indices_right = RIGHT_IRIS + ADJACENT_RIGHT_EYELID_PART
                    right_eye_points = lms_s[indices_right, :]
                    right_eye_points_model = [at.to_m2(p) for p in right_eye_points]
                    self.right_detector.update(right_eye_points_model, timestamp_ms)

                    left_gaze_vector = None
                    right_gaze_vector = None
                    left_status = "Calibrating..."
                    right_status = "Calibrating..."

                    # Process left eye
                    if self.left_detector.center_detected:
                        left_center = at.to_m1(self.left_detector.eye_center)
                        left_pupil = lms_s[LEFT_PUPIL]
                        left_gaze_vector = left_pupil - left_center
                        left_out_of_bounds = self.is_gaze_out_of_bounds(left_gaze_vector, True)
                        left_status = "Out of bounds" if left_out_of_bounds else "Normal"
                        
                        if self.vis_options:
                            left_pupil_px = relative(left_pupil[:2], frame.shape)
                            self.draw_gaze_vector(frame, left_pupil_px, left_gaze_vector, left_out_of_bounds)

                        if left_out_of_bounds:
                            self.log_cheating_event("Gaze away - left eye", frame)

                    # Process right eye
                    if self.right_detector.center_detected:
                        right_center = at.to_m1(self.right_detector.eye_center)
                        right_pupil = lms_s[RIGHT_PUPIL]
                        right_gaze_vector = right_pupil - right_center
                        right_out_of_bounds = self.is_gaze_out_of_bounds(right_gaze_vector, False)
                        right_status = "Out of bounds" if right_out_of_bounds else "Normal"
                        
                        if self.vis_options:
                            right_pupil_px = relative(right_pupil[:2], frame.shape)
                            self.draw_gaze_vector(frame, right_pupil_px, right_gaze_vector, right_out_of_bounds)

                        if right_out_of_bounds:
                            self.log_cheating_event("Gaze away - right eye", frame)

                    # Draw status and handle callback
                    if self.vis_options:
                        self.draw_gaze_status(frame, left_status, right_status)

                    if self.callback and (left_gaze_vector is not None or right_gaze_vector is not None):
                        await self.callback(left_gaze_vector, right_gaze_vector)

                cv2.imshow('LaserGaze', frame)
                if cv2.waitKey(5) & 0xFF == 27:
                    break

            cap.release()
            cv2.destroyAllWindows()



================================================
File: VisualizationOptions.py
================================================
# -----------------------------------------------------------------------------------
# Company: TensorSense
# Project: LaserGaze
# File: VisualizationOptions.py
# Description: A class to store visualization settings for rendering gaze vectors
#              on video frames.
# Author: Sergey Kuldin
# -----------------------------------------------------------------------------------


class VisualizationOptions:
    def __init__(self):
        self.color = (0, 255, 0)  # Green color for normal visualization
        self.warning_color = (0, 0, 255)  # Red color for warnings
        self.peripheral_color = (0, 255, 255)  # Yellow color for peripheral zone
        self.line_thickness = 2
        self.box_thickness = 1
        self.text_scale = 0.5


================================================
File: face_landmarker.task
================================================
[Non-text file]


================================================
File: face_model.py
================================================
# -----------------------------------------------------------------------------------
# Company: TensorSense
# Project: LaserGaze
# File: face_model.py
# Description: A universal face model used for creating a head coordinate space. This
#              module contains definitions of key facial points and is used for
#              calculations within the context of head movement recognition and
#              analysis.
# Author: Sergey Kuldin
# -----------------------------------------------------------------------------------

import numpy as np

INTERNAL_EYES_CORNERS_MODEL = np.array([
    [-0.035, -0.05, 0],
    [0.035, -0.05, 0]
])

OUTER_EYES_CORNERS_MODEL = np.array([
    [-0.09, -0.057, 0.01],
    [0.09, -0.057, 0.01]
])

OUTER_HEAD_POINTS_MODEL = np.array([
    [-0.145, -0.1, 0.1],
    [0.145, -0.1, 0.1]
])

NOSE_BRIDGE_MODEL = np.array([
    [0, -0.0319, -0.0432]
])

NOSE_TIP_MODEL = np.array([
    [0, 0.088, -0.071]
])

BASE_FACE_MODEL = np.vstack((
    INTERNAL_EYES_CORNERS_MODEL,
    OUTER_EYES_CORNERS_MODEL,
    OUTER_HEAD_POINTS_MODEL,
    NOSE_BRIDGE_MODEL,
    NOSE_TIP_MODEL
))

DEFAULT_LEFT_EYE_CENTER_MODEL = (np.array(INTERNAL_EYES_CORNERS_MODEL[0]) + np.array(OUTER_EYES_CORNERS_MODEL[0])) * 0.5
DEFAULT_LEFT_EYE_CENTER_MODEL[1] -= 0.009
DEFAULT_LEFT_EYE_CENTER_MODEL[2] = 0.02

DEFAULT_RIGHT_EYE_CENTER_MODEL = (np.array(INTERNAL_EYES_CORNERS_MODEL[1]) + np.array(OUTER_EYES_CORNERS_MODEL[1])) * 0.5
DEFAULT_RIGHT_EYE_CENTER_MODEL[1] -= 0.009
DEFAULT_RIGHT_EYE_CENTER_MODEL[2] = 0.02

DEFAULT_EYE_RADIUS = 0.02



================================================
File: helper.py
================================================
import cv2
import numpy as np

import torch
import torch.nn as nn
from torch.utils.data import DataLoader
from torchvision import transforms


from resnet import resnet18, resnet34, resnet50

def get_model(arch, bins, pretrained=False, inference_mode=False):
    """Return the model based on the specified architecture."""
    if arch == 'resnet18':
        model = resnet18(pretrained=pretrained, num_classes=bins)
    elif arch == 'resnet34':
        model = resnet34(pretrained=pretrained, num_classes=bins)
    elif arch == 'resnet50':
        model = resnet50(pretrained=pretrained, num_classes=bins)
    # elif arch == "mobilenetv2":
    #     model = mobilenet_v2(pretrained=pretrained, num_classes=bins)
    # elif arch == "mobileone_s0":
    #     model = mobileone_s0(pretrained=pretrained, num_classes=bins, inference_mode=inference_mode)
    # elif arch == "mobileone_s1":
    #     model = mobileone_s1(pretrained=pretrained, num_classes=bins, inference_mode=inference_mode)
    # elif arch == "mobileone_s2":
    #     model = mobileone_s2(pretrained=pretrained, num_classes=bins, inference_mode=inference_mode)
    # elif arch == "mobileone_s3":
    #     model = mobileone_s3(pretrained=pretrained, num_classes=bins, inference_mode=inference_mode)
    # elif arch == "mobileone_s4":
    #     model = mobileone_s4(pretrained=pretrained, num_classes=bins, inference_mode=inference_mode)
    else:
        raise ValueError(f"Please choose available model architecture, currently chosen: {arch}")
    return model


def angular_error(gaze_vector, label_vector):
    dot_product = np.dot(gaze_vector, label_vector)
    norm_product = np.linalg.norm(gaze_vector) * np.linalg.norm(label_vector)
    cosine_similarity = min(dot_product / norm_product, 0.9999999)

    return np.degrees(np.arccos(cosine_similarity))


def gaze_to_3d(gaze):
    yaw = gaze[0]   # Horizontal angle
    pitch = gaze[1]  # Vertical angle

    gaze_vector = np.zeros(3)
    gaze_vector[0] = -np.cos(pitch) * np.sin(yaw)
    gaze_vector[1] = -np.sin(pitch)
    gaze_vector[2] = -np.cos(pitch) * np.cos(yaw)

    return gaze_vector




================================================
File: landmarks.py
================================================
# -----------------------------------------------------------------------------------
# Company: TensorSense
# Project: LaserGaze
# File: landmarks.py
# Description: This file defines a series of key facial landmark indices used for gaze
#              estimation and other facial analysis tasks within the LaserGaze project.
#              It includes definitions for the positions of outer head points, nose
#              bridge, nose tip, irises, pupils, and both internal and external eye corners.
#              Additionally, adjacent eyelid parts are defined for more detailed eye
#              tracking. Utility functions for converting normalized landmark coordinates
#              to pixel coordinates on images are also provided. These landmarks and
#              utilities facilitate precise tracking and manipulation of facial features
#              in real-time video processing.
# Author: Sergey Kuldin
# -----------------------------------------------------------------------------------

OUTER_HEAD_POINTS = [162, 389]
NOSE_BRIDGE = 6
NOSE_TIP = 4

LEFT_IRIS = [469, 470, 471, 472]
LEFT_PUPIL = 468

RIGHT_IRIS = [474, 475, 476, 477]
RIGHT_PUPIL = 473

INTERNAL_EYES_CORNERS = [155, 362]
OUTER_EYES_CORNERS = [33, 263]

ADJACENT_LEFT_EYELID_PART = [160, 159, 158, 163, 144, 145, 153]
ADJACENT_RIGHT_EYELID_PART = [387, 386, 385, 390, 373, 374, 380]

BASE_LANDMARKS = INTERNAL_EYES_CORNERS + OUTER_EYES_CORNERS + OUTER_HEAD_POINTS + [NOSE_BRIDGE] + [NOSE_TIP]

relative = lambda landmark, shape: (int(landmark[0] * shape[1]), int(landmark[1] * shape[0]))
relativeT = lambda landmark, shape: (int(landmark[0] * shape[1]), int(landmark[1] * shape[0]), 0)


================================================
File: main.py
================================================
import cv2
import mediapipe as mp
from ultralytics import YOLO

from GazeProcessor import GazeProcessor
from VisualizationOptions import VisualizationOptions
import asyncio

async def gaze_vectors_collected(left, right):
    print(f"left: {left}, right: {right}")

async def main():
    vo = VisualizationOptions()
    gp = GazeProcessor(camera_idx = 1, visualization_options=vo, callback=gaze_vectors_collected)
    await gp.start()

if __name__ == "__main__":
    asyncio.run(main())



# video = cv2.VideoCapture(1)

# mp_face_mesh = mp.solutions.face_mesh
# face_mesh = mp_face_mesh.FaceMesh(static_image_mode=False, max_num_faces=1)

# model = YOLO('yolo11l.pt')

# while True:
#     ret, frame = video.read()
#     if not ret:
#         break

#     rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
#     results = face_mesh.process(rgb)

#     detections = model.predict(source=frame, classes=[67], stream=True)

#     for r in detections:
#         for box in r.boxes:
#             x1, y1, x2, y2 = map(int, box.xyxy[0])
#             conf = box.conf[0].item()
#             if conf > 0.5:
#                 cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 2)
#                 cv2.putText(frame, f'Phone: {conf:.2f}', (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 0), 2)

#     if results.multi_face_landmarks:
#         for face_landmarks in results.multi_face_landmarks:
#             left_eye = face_landmarks.landmark[33]
#             right_eye = face_landmarks.landmark[263]
#             nose = face_landmarks.landmark[1]

#             image_height, image_width, _ = frame.shape
#             lx, ly = int(left_eye.x * image_width), int(left_eye.y * image_height)
#             rx, ry = int(right_eye.x * image_width), int(right_eye.y * image_height)
#             nx, ny = int(nose.x * image_width), int(nose.y * image_height)

#             eye_center_x = (lx + rx) // 2
#             eye_center_y = (ly + ry) // 2

#             gaze_vector_x = nx - eye_center_x
#             gaze_vector_y = ny - eye_center_y

#             scale = 2
#             end_point_x = int(eye_center_x + scale * gaze_vector_x)
#             end_point_y = int(eye_center_y + scale * gaze_vector_y)

#             cv2.arrowedLine(frame, (eye_center_x, eye_center_y), (end_point_x, end_point_y), (255, 0, 0), 2)


#     cv2.imshow('Frame', frame)
#     if cv2.waitKey(1) & 0xFF == 27:
#         break

# video.release()
# cv2.destroyAllWindows()



================================================
File: requirements.txt
================================================
ultralytics
mediapipe
scipy
torch --index-url https://download.pytorch.org/whl/cu126



================================================
File: resnet.py
================================================
import torch
from torch import nn, Tensor
from torchvision.models import ResNet18_Weights, ResNet34_Weights, ResNet50_Weights

from typing import Any, Callable, List, Optional, Type, Tuple


__all__ = ["resnet18", "resnet34", "resnet50"]


def conv3x3(in_channels: int, out_channels: int, stride: int = 1, groups: int = 1, dilation: int = 1) -> nn.Conv2d:
    """3x3 convolution with padding"""
    return nn.Conv2d(
        in_channels,
        out_channels,
        kernel_size=3,
        stride=stride,
        padding=dilation,
        groups=groups,
        bias=False,
        dilation=dilation,
    )


def conv1x1(in_channels: int, out_channels: int, stride: int = 1) -> nn.Conv2d:
    """1x1 convolution"""
    return nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride, bias=False)


class BasicBlock(nn.Module):
    expansion: int = 1

    def __init__(
            self,
            in_channels: int,
            out_channels: int,
            stride: int = 1,
            downsample: Optional[nn.Module] = None,
            groups: int = 1,
            base_width: int = 64,
            dilation: int = 1,
            norm_layer: Optional[Callable[..., nn.Module]] = None,
    ) -> None:
        super().__init__()
        if norm_layer is None:
            norm_layer = nn.BatchNorm2d
        if groups != 1 or base_width != 64:
            raise ValueError("BasicBlock only supports groups=1 and base_width=64")
        if dilation > 1:
            raise NotImplementedError("Dilation > 1 not supported in BasicBlock")
        # Both self.conv1 and self.downsample layers downsample the input when stride != 1
        self.conv1 = conv3x3(in_channels, out_channels, stride)
        self.bn1 = norm_layer(out_channels)
        self.relu = nn.ReLU(inplace=True)
        self.conv2 = conv3x3(out_channels, out_channels)
        self.bn2 = norm_layer(out_channels)
        self.downsample = downsample
        self.stride = stride

    def forward(self, x: Tensor) -> Tensor:
        identity = x

        out = self.conv1(x)
        out = self.bn1(out)
        out = self.relu(out)

        out = self.conv2(out)
        out = self.bn2(out)

        if self.downsample is not None:
            identity = self.downsample(x)

        out += identity
        out = self.relu(out)

        return out


class Bottleneck(nn.Module):
    expansion: int = 4

    def __init__(
        self,
        inplanes: int,
        planes: int,
        stride: int = 1,
        downsample: Optional[nn.Module] = None,
        groups: int = 1,
        base_width: int = 64,
        dilation: int = 1,
        norm_layer: Optional[Callable[..., nn.Module]] = None,
    ) -> None:
        super().__init__()
        if norm_layer is None:
            norm_layer = nn.BatchNorm2d
        width = int(planes * (base_width / 64.0)) * groups
        # Both self.conv2 and self.downsample layers downsample the input when stride != 1
        self.conv1 = conv1x1(inplanes, width)
        self.bn1 = norm_layer(width)
        self.conv2 = conv3x3(width, width, stride, groups, dilation)
        self.bn2 = norm_layer(width)
        self.conv3 = conv1x1(width, planes * self.expansion)
        self.bn3 = norm_layer(planes * self.expansion)
        self.relu = nn.ReLU(inplace=True)
        self.downsample = downsample
        self.stride = stride

    def forward(self, x: Tensor) -> Tensor:
        identity = x

        out = self.conv1(x)
        out = self.bn1(out)
        out = self.relu(out)

        out = self.conv2(out)
        out = self.bn2(out)
        out = self.relu(out)

        out = self.conv3(out)
        out = self.bn3(out)

        if self.downsample is not None:
            identity = self.downsample(x)

        out += identity
        out = self.relu(out)

        return out


class ResNet(nn.Module):
    def __init__(
            self,
            block: Type[BasicBlock | Bottleneck],
            layers: List[int],
            num_classes: int = 1000,
            groups: int = 1,
            width_per_group: int = 64,
            replace_stride_with_dilation: Optional[List[bool]] = None,
            norm_layer: Optional[Callable[..., nn.Module]] = None,
    ) -> None:
        super().__init__()
        if norm_layer is None:
            norm_layer = nn.BatchNorm2d
        self._norm_layer = norm_layer

        self.in_channels = 64
        self.dilation = 1
        if replace_stride_with_dilation is None:
            # each element in the tuple indicates if we should replace
            # the 2x2 stride with a dilated convolution instead
            replace_stride_with_dilation = [False, False, False]
        if len(replace_stride_with_dilation) != 3:
            raise ValueError(
                "replace_stride_with_dilation should be None "
                f"or a 3-element tuple, got {replace_stride_with_dilation}"
            )
        self.groups = groups
        self.base_width = width_per_group
        self.conv1 = nn.Conv2d(3, self.in_channels, kernel_size=7, stride=2, padding=3, bias=False)
        self.bn1 = norm_layer(self.in_channels)
        self.relu = nn.ReLU(inplace=True)
        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)
        self.layer1 = self._make_layer(block, 64, layers[0])
        self.layer2 = self._make_layer(block, 128, layers[1], stride=2, dilate=replace_stride_with_dilation[0])
        self.layer3 = self._make_layer(block, 256, layers[2], stride=2, dilate=replace_stride_with_dilation[1])
        self.layer4 = self._make_layer(block, 512, layers[3], stride=2, dilate=replace_stride_with_dilation[2])
        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))

        # yaw and pitch
        self.fc_yaw = nn.Linear(512 * block.expansion, num_classes)
        self.fc_pitch = nn.Linear(512 * block.expansion, num_classes)

        # Original FC Layer for ResNet
        # self.fc = nn.Linear(512 * block.expansion, num_classes)

        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                nn.init.kaiming_normal_(m.weight, mode="fan_out", nonlinearity="relu")
            elif isinstance(m, (nn.BatchNorm2d, nn.GroupNorm)):
                nn.init.constant_(m.weight, 1)
                nn.init.constant_(m.bias, 0)

    def _make_layer(
            self,
            block: Type[BasicBlock | Bottleneck],
            planes: int,
            blocks: int,
            stride: int = 1,
            dilate: bool = False,
    ) -> nn.Sequential:
        norm_layer = self._norm_layer
        downsample = None
        previous_dilation = self.dilation
        if dilate:
            self.dilation *= stride
            stride = 1
        if stride != 1 or self.in_channels != planes * block.expansion:
            downsample = nn.Sequential(
                conv1x1(self.in_channels, planes * block.expansion, stride),
                norm_layer(planes * block.expansion),
            )

        layers = []
        layers.append(
            block(
                self.in_channels,
                planes,
                stride,
                downsample,
                self.groups,
                self.base_width,
                previous_dilation,
                norm_layer
            )
        )
        self.in_channels = planes * block.expansion
        for _ in range(1, blocks):
            layers.append(
                block(
                    self.in_channels,
                    planes,
                    groups=self.groups,
                    base_width=self.base_width,
                    dilation=self.dilation,
                    norm_layer=norm_layer,
                )
            )

        return nn.Sequential(*layers)

    def forward(self, x: Tensor) -> Tuple[Tensor, Tensor, Tensor]:
        x = self.conv1(x)
        x = self.bn1(x)
        x = self.relu(x)
        x = self.maxpool(x)

        x = self.layer1(x)  # 1/4
        x = self.layer2(x)  # 1/8
        x = self.layer3(x)  # 1/16
        x = self.layer4(x)  # 1/32

        x = self.avgpool(x)
        x = torch.flatten(x, 1)

        # Original FC Layer for ResNet
        # x = self.fc(x)
        yaw = self.fc_yaw(x)
        pitch = self.fc_pitch(x)

        return pitch, yaw


def load_filtered_state_dict(model, state_dict):
    """Update the model's state dictionary with filtered parameters.

    Args:
        model: The model instance to update (must have `state_dict` and `load_state_dict` methods).
        state_dict: A dictionary of parameters to load into the model.
    """
    current_model_dict = model.state_dict()
    filtered_state_dict = {key: value for key, value in state_dict.items() if key in current_model_dict}
    current_model_dict.update(filtered_state_dict)
    model.load_state_dict(current_model_dict)


def _resnet(block: Type[BasicBlock], layers: List[int], weights: Optional[ResNet34_Weights], progress: bool, **kwargs: Any) -> ResNet:
    model = ResNet(block, layers, **kwargs)

    if weights is not None:
        state_dict = weights.get_state_dict(progress=progress, check_hash=True)
        load_filtered_state_dict(model, state_dict)

    return model


def resnet18(*, pretrained: bool = True, progress: bool = True, **kwargs: Any) -> ResNet:
    if pretrained:
        weights = ResNet18_Weights.DEFAULT
    else:
        weights = None
    return _resnet(BasicBlock, [2, 2, 2, 2], weights, progress, **kwargs)


def resnet34(*, pretrained: bool = True, progress: bool = True, **kwargs: Any) -> ResNet:
    if pretrained:
        weights = ResNet34_Weights.DEFAULT
    else:
        weights = None
    return _resnet(BasicBlock, [3, 4, 6, 3], weights, progress, **kwargs)


def resnet50(*, pretrained: bool = True, progress: bool = True, **kwargs: Any) -> ResNet:
    if pretrained:
        weights = ResNet50_Weights.DEFAULT
    else:
        weights = None

    return _resnet(Bottleneck, [3, 4, 6, 3], weights, progress, **kwargs)


================================================
File: resnet34.pt
================================================
[Non-text file]


================================================
File: yolo11l.pt
================================================
[Non-text file]


================================================
File: cheating_logs/log.txt
================================================
